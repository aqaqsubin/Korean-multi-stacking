{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "from os.path import join as pjoin\n",
    "from glob import iglob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers', 'encoder', 'ff_actv', 'use_interval', 'rnn_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/data/ksb/'\n",
    "bert_root_path = pjoin(root_path, 'BertSum/PreSumm')\n",
    "bert_model_dir = pjoin(bert_root_path, 'models')\n",
    "\n",
    "data_dir = pjoin(root_path, 'cnn-dailymail/finished_files')\n",
    "\n",
    "three_data_dir = pjoin(root_path, 'three-mat')\n",
    "three_data_test = pjoin(three_data_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function 비교  \n",
    "\n",
    "*Trained Model parameter 필요*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_similarity(inputs, summaries):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    cos_similarity_list = []\n",
    "    for input_, summary_ in zip(inputs, summaries):\n",
    "        try:\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform([input_, summary_])\n",
    "\n",
    "            similarity = cosine_similarity(tfidf_matrix[0] , tfidf_matrix[1])[0][0]\n",
    "        except ValueError:\n",
    "            similarity = 0.0\n",
    "            \n",
    "        cos_similarity_list.append(similarity)\n",
    "\n",
    "    return cos_similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "\n",
    "data_list = []\n",
    "for data_p in iglob(pjoin(three_data_test, '**.json'), recursive=False):\n",
    "    \n",
    "    with open(data_p,'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        data_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Origin candidate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tok = BertTokenizer.from_pretrained('bert-base-uncased', verbose=False)\n",
    "\n",
    "def bert_encode(x, max_len=-1):\n",
    "    cls_token_id = tok.cls_token_id\n",
    "    sep_token_id = tok.sep_token_id\n",
    "\n",
    "    _ids = tok.encode(x, add_special_tokens=False)\n",
    "    ids = [cls_token_id] # [CLS]\n",
    "    if max_len > 0:\n",
    "        ids.extend(_ids[:max_len - 2])\n",
    "    else:\n",
    "        ids.extend(_ids[:512 - 2])\n",
    "    ids.append(sep_token_id) # [SEP], meaning end of sentence\n",
    "    return ids\n",
    "\n",
    "def bert_decode(x):\n",
    "    result = tok.decode(x, skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_trigram(src, tgt):\n",
    "    assert len(tgt) > 2 and len(src) > 2\n",
    "        \n",
    "    tgt_trigrams = [(tgt[i-1],tgt[i],tgt[i+1]) for i in range(1,len(tgt)-1)]\n",
    "    src_trigrams = [(src[i-1],src[i],src[i+1]) for i in range(1,len(src)-1)]\n",
    "    \n",
    "    for src_tri in src_trigrams:\n",
    "        if src_tri in tgt_trigrams:\n",
    "            return True ## Detect trigram overlapped with target\n",
    "        \n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_4_gram(src, tgt):\n",
    "    assert len(tgt) > 3 and len(src) > 3\n",
    "    tgt_4_grams = [(tgt[i-2], tgt[i-1],tgt[i],tgt[i+1]) for i in range(2,len(tgt)-1)]\n",
    "    src_4_grams = [(src[i-2], src[i-1],src[i],src[i+1]) for i in range(2,len(src)-1)]\n",
    "    \n",
    "    for src_gram in src_4_grams:\n",
    "        if src_gram in tgt_4_grams:\n",
    "            return True ## Detect 4-gram overlapped with target\n",
    "        \n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_5_gram(src, tgt):\n",
    "    assert len(tgt) > 4 and len(src) > 4\n",
    "    tgt_5_grams = [(tgt[i-2], tgt[i-1],tgt[i],tgt[i+1], tgt[i+2]) for i in range(2,len(tgt)-2)]\n",
    "    src_5_grams = [(src[i-2], src[i-1],src[i],src[i+1], src[i+2]) for i in range(2,len(src)-2)]\n",
    "    \n",
    "    for src_gram in src_5_grams:\n",
    "        if src_gram in tgt_5_grams:\n",
    "            return True ## Detect 5-gram overlapped with target\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ngram_list(src, tgt_list, n_gram='trigram'):\n",
    "    \n",
    "    if n_gram =='trigram':\n",
    "        return sum([detect_trigram(src, tgt) for tgt in tgt_list]) > 0\n",
    "    elif n_gram =='4-gram':\n",
    "        return sum([detect_4_gram(src, tgt) for tgt in tgt_list]) > 0\n",
    "    else :\n",
    "        return sum([detect_4_gram(src, tgt) for tgt in tgt_list])>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_set(sent_set, reference=None, n_gram='trigram'):\n",
    "    \n",
    "    assert n_gram in ['trigram', '4-gram','5-gram']\n",
    "        \n",
    "    if n_gram == 'trigram':\n",
    "        detect_ngram = detect_trigram\n",
    "    elif n_gram == '4-gram':\n",
    "        detect_ngram = detect_4_gram\n",
    "    else:\n",
    "        detect_ngram = detect_5_gram\n",
    "        \n",
    "    \n",
    "    possible_set_ids = []\n",
    "    \n",
    "    for sent_id, sent in sent_set:\n",
    "        possible_2_sent_idx = []\n",
    "        \n",
    "        # number of summary sentences = 2\n",
    "        for tgt_sent_id, tgt_sent in sent_set[sent_id+1:]:\n",
    "            \n",
    "            # Detect n-gram (default= trigram) \n",
    "            if not detect_ngram(src=sent, tgt=tgt_sent):\n",
    "                possible_2_sent_idx.append(set([sent_id, tgt_sent_id]))\n",
    "                \n",
    "        possible_3_sent_idx = copy.deepcopy(possible_2_sent_idx)\n",
    "        \n",
    "        # number of summary sentences = 3\n",
    "        for tgt_sent_id, tgt_sent in sent_set[sent_id+1:]:\n",
    "            for poss_sent_ids in possible_2_sent_idx:\n",
    "                \n",
    "                poss_sent = [sent_set[ids][1] for ids in poss_sent_ids]\n",
    "                if not detect_ngram_list(src=tgt_sent, tgt_list=poss_sent, n_gram=n_gram):\n",
    "                    poss_3_ids = copy.deepcopy(poss_sent_ids)\n",
    "                    poss_3_ids.add(tgt_sent_id)\n",
    "                    \n",
    "                    possible_3_sent_idx.append(poss_3_ids)\n",
    "                    \n",
    "\n",
    "        possible_sent_idx = possible_2_sent_idx + possible_3_sent_idx\n",
    "        \n",
    "        for ids in possible_sent_idx:\n",
    "            if not ids in possible_set_ids:\n",
    "                possible_set_ids.append(ids)\n",
    "\n",
    "    return possible_set_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylcs\n",
    "\n",
    "def compute_txt_redundancy_score(candidate_id):\n",
    "\n",
    "    cand_num = len(candidate_id)\n",
    "    \n",
    "    score = torch.zeros([cand_num], dtype=torch.float64)\n",
    "        \n",
    "    def _compute_redundancy(cand):\n",
    "        redundancy = 0.0\n",
    "        \n",
    "        for i, src_sen in enumerate(cand):\n",
    "            for j, tgt_sen in enumerate(cand[i+1:]):\n",
    "                if i != j:\n",
    "                    lcs_val = pylcs.lcs(src_sen, tgt_sen)\n",
    "                    redundancy += lcs_val \n",
    "        \n",
    "        sents_len = sum([len(s) for sents in cand for s in sents])\n",
    "        return redundancy / sents_len\n",
    "\n",
    "    for i in range(cand_num):\n",
    "        score[i] = np.mean(_compute_redundancy(candidate_id[i]))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_path = pjoin(three_data_dir,'reconstructed_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin ROUGE : [0.4474, 0.5116, 0.4478]\n",
      "New Candidate ROUGE : [0.5333, 0.5333, 0.5333, 0.5128, 0.5128, 0.5128, 0.48, 0.48, 0.4762, 0.4762, 0.4762, 0.4687, 0.4561, 0.4561, 0.4561, 0.4533, 0.4533, 0.4533, 0.4533, 0.4478]\n",
      "\n",
      "Origin ROUGE : [0.4138, 0.3373, 0.375]\n",
      "New Candidate ROUGE : [0.4211, 0.4186, 0.4051, 0.4045, 0.3918, 0.3908, 0.38, 0.3778, 0.3678, 0.3556, 0.3556, 0.3441, 0.34, 0.3301, 0.3297, 0.3288, 0.321, 0.3191, 0.3158, 0.3143]\n",
      "\n",
      "Origin ROUGE : [0.3208, 0.3368, 0.2936]\n",
      "New Candidate ROUGE : [0.3956, 0.3956, 0.3908, 0.3908, 0.3846, 0.3846, 0.3488, 0.3368, 0.3368, 0.3368, 0.3368, 0.3299, 0.3299, 0.3297, 0.3297, 0.3243, 0.3226, 0.3226, 0.321, 0.321]\n",
      "\n",
      "Origin ROUGE : [0.4286, 0.4737, 0.4717]\n",
      "New Candidate ROUGE : [0.5306, 0.52, 0.4884, 0.48, 0.4792, 0.4762, 0.4752, 0.4752, 0.4694, 0.4694, 0.4646, 0.4646, 0.4598, 0.4583, 0.4565, 0.4471, 0.4444, 0.4444, 0.44, 0.4356]\n",
      "\n",
      "Origin ROUGE : [0.1791, 0.3023, 0.1724]\n",
      "New Candidate ROUGE : [0.5306, 0.4444, 0.4444, 0.4262, 0.4211, 0.3939, 0.3939, 0.3939, 0.381, 0.3774, 0.3714, 0.3662, 0.3516, 0.3478, 0.3467, 0.3462, 0.3377, 0.3288, 0.3243, 0.3243]\n",
      "\n",
      "Origin ROUGE : [0.4468, 0.5345, 0.4381]\n",
      "New Candidate ROUGE : [0.5155, 0.5091, 0.5, 0.495, 0.495, 0.4912, 0.4912, 0.4792, 0.4792, 0.4505, 0.4386, 0.4386, 0.4348, 0.4286, 0.4219, 0.4219, 0.4211, 0.4211, 0.4112, 0.4037]\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ff7fef557e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_cand_rouge\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin_cand_rouge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0morigin_cand_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin_cand_rouge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0morigin_cand_redun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_txt_redundancy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcand\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mreconst_cand_redun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_txt_redundancy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcand\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreconstructed_candidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(new_data_path, 'w', encoding='utf-8') as f:\n",
    "    writer = jsonlines.Writer(f)\n",
    "    \n",
    "    for data in data_list:\n",
    "        candidates = data['candidates']\n",
    "        article = data['article']\n",
    "        abstract = data[\"abstract\"]\n",
    "\n",
    "\n",
    "        summaries = [cand[0] for cand in candidates]    \n",
    "        encoded_cand_set = [[bert_encode(s, 180) for s in cs] for cs in summaries]\n",
    "        threshold = min([cand[1] for cand in candidates]) \n",
    "\n",
    "        sent_set = []\n",
    "\n",
    "        for i, encoded_cand in enumerate(encoded_cand_set):\n",
    "            for j, encoded_sent in enumerate(encoded_cand):\n",
    "\n",
    "                sent_id = sum([len(prev) for prev in encoded_cand_set[:i]])+j\n",
    "                sent_set.append((sent_id, encoded_sent))\n",
    "\n",
    "        reduced_cand_ids = get_candidate_set(sent_set)\n",
    "        reduced_cand_sents = [[sent_set[i][1] for i in ids] for ids in reduced_cand_ids]\n",
    "        reduced_cand_set_dec = [[bert_decode(x) for x in cand] for cand in reduced_cand_sents]\n",
    "\n",
    "\n",
    "        # Drop candidate which has lower score than threshold\n",
    "        rouge_cands_set = []\n",
    "        for c in reduced_cand_set_dec:\n",
    "            scores = rouge.get_scores(' '.join(abstract), ' '.join(c))[0]\n",
    "            score = scores['rouge-l']['f']\n",
    "\n",
    "            rouge_cands_set.append((score, c))\n",
    "\n",
    "        rouge_cands_set = sorted(rouge_cands_set, key=lambda x: x[0], reverse=True)\n",
    "        reconstructed_candidates = [s[-1] for s in rouge_cands_set[:20]]\n",
    "        \n",
    "        origin_cand_rouge = [round(rouge.get_scores(' '.join(abstract), ' '.join(cand[0]))[0]['rouge-l']['f'],4) for cand in candidates]\n",
    "        new_cand_rouge = [round(rouge.get_scores(' '.join(abstract), ' '.join(cand))[0]['rouge-l']['f'],4) for cand in reconstructed_candidates]\n",
    "        \n",
    "        print(\"Origin ROUGE : {}\".format(origin_cand_rouge))\n",
    "        print(\"New Candidate ROUGE : {}\\n\".format(new_cand_rouge))\n",
    "        \n",
    "        if max(new_cand_rouge) <= max(origin_cand_rouge):\n",
    "            origin_cand_id = np.argmax(origin_cand_rouge)\n",
    "            origin_cand_redun = compute_txt_redundancy_score([cand[0] for cand in candidates]).item()\n",
    "            reconst_cand_redun = compute_txt_redundancy_score([cand for cand in reconstructed_candidates]).item()\n",
    "            \n",
    "            print(origin_cand_redun)\n",
    "            print(reconst_cand_redun)\n",
    "            assert origin_cand_redun[origin_cand_id] > max(reconst_cand_redun)\n",
    "            \n",
    "        \n",
    "        new_data = {'article':article, 'candidates':candidates, 'abstract':abstract, 'new_candidates': reconstructed_candidates}\n",
    "        writer.write(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Origin Redundancy score : {}\".format(round(np.mean(origin_redun), 4)))\n",
    "print(\"Origin cosine similarity between document and summaries : {}\".format(round(np.mean(origin_doc_sims), 4)))\n",
    "print(\"Origin ROUGE score between reference and summaries : {}\".format(round(np.mean(origin_ref_rouges), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Origin Redundancy score : {}\".format(round(np.mean(refine_redun), 4)))\n",
    "print(\"Origin cosine similarity between document and summaries : {}\".format(round(np.mean(refine_doc_sims), 4)))\n",
    "print(\"Origin ROUGE score between reference and summaries : {}\".format(round(np.mean(refine_ref_rouges), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
